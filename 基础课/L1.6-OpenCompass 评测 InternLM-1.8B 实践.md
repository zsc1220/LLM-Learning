# OpenCompass 评测 InternLM-1.8B 实践

## 1 大模型评测概述
当前大模型层出不穷，对其进行准确、全面的评测至关重要。OpenCompass 作为一款强大的评测工具，为我们提供了有效的手段来评估大模型的性能和能力。
![alt text](img6/为什么需要评测.png)
![alt text](img6/通过能力评测促进大模型发展.png)
### 1.1 OpenCompass介绍
上海人工智能实验室科学家团队正式发布了大模型开源开放评测体系 “司南” (OpenCompass2.0)，用于为大语言模型、多模态模型等提供一站式评测服务。其主要特点如下：
- 开源可复现：提供公平、公开、可复现的大模型评测方案
- 面的能力维度：五大维度设计，提供 70+ 个数据集约 40 万题的的模型评测方案，全面评估模型能力
- 丰富的模型支持：已支持 20+ HuggingFace 及 API 模型
- 分布式高效评测：一行命令实现任务分割和分布式评测，数小时即可完成千亿模型全量评测
- 多样化评测范式：支持零样本、小样本及思维链评测，结合标准型或对话型提示词模板，轻松激发各种模型最大性能
- 灵活化拓展：想增加新模型或数据集？想要自定义更高级的任务分割策略，甚至接入新的集群管理系统？OpenCompass 的一切均可轻松扩展！
![alt text](img6/OpenCompass评测体系开源历程.png)
### 1.2 评测对象
本算法库的主要评测对象为语言大模型与多模态大模型。我们以语言大模型为例介绍评测的具体模型类型。
- 基座模型：一般是经过海量的文本数据以自监督学习的方式进行训练获得的模型（如OpenAI的GPT-3，Meta的LLaMA），往往具有强大的文字续写能力。
- 对话模型：一般是在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（如OpenAI的ChatGPT、上海人工智能实验室的书生·浦语），能理解人类指令，具有较强的对话能力。
![alt text](img6/大语言模型需要评测什么.png)
### 1.2 大模型评测领域的挑战
![alt text](img6/大模型评测中的挑战.png)
![alt text](img6/大模型评测领域的挑战.png)

## 2 技术方案
### 2.1 评测方法
OpenCompass 采取客观评测与主观评测相结合的方法。针对具有确定性答案的能力维度和场景，通过构造丰富完善的评测集，对模型能力进行综合评价。针对体现模型能力的开放式或半开放式的问题、模型安全问题等，采用主客观相结合的评测方式。
- 客观评测
针对具有标准答案的客观问题，我们可以通过使用定量指标比较模型的输出与标准答案的差异，并根据结果衡量模型的性能。
    - 判别式评测：该评测方式基于将问题与候选答案组合在一起，计算模型在所有组合上的困惑度（perplexity），并选择困惑度最小的答案作为模型的最终输出。例如，若模型在 问题? 答案1 上的困惑度为 0.1，在 问题? 答案2 上的困惑度为 0.2，最终我们会选择 答案1 作为模型的输出。
    - 生成式评测：该评测方式主要用于生成类任务，如语言翻译、程序生成、逻辑分析题等。具体实践时，使用问题作为模型的原始输入，并留白答案区域待模型进行后续补全。我们通常还需要对其输出进行后处理，以保证输出满足数据集的要求。
- 主观评测
针对如模型安全和模型语言能力的评测，在具体实践中，我们提前基于模型的能力维度构建主观测试问题集合，并将不同模型对于同一问题的不同回复展现给受试者，收集受试者基于主观感受的评分。由于主观测试成本高昂，本方案同时也采用使用性能优异的大语言模拟人类进行主观打分（模型评价模型）。 
![alt text](img6/客观评测与主观评测.png)
### 2.2 主流大模型评测框架
![alt text](img6/主流大模型评测框架.png)
OpenCompass评测框架：
![alt text](img6/OpenCompass评测框架.png)
### 2.3 OpenCompass工具平台架构
![alt text](img6/OpenCompass评测工具架构图.png)
- 模型层：大模型评测所涉及的主要模型种类，OpenCompass 以基座模型和对话模型作为重点评测对象。支持国内外主流得大部分模型。
- 能力层：OpenCompass 从本方案从通用能力和特色能力两个方面来进行评测维度设计。在模型通用能力方面，从语言、知识、理解、推理、安全等多个能力维度进行评测。在特色能力方面，从长文本、代码、工具、知识增强等维度进行评测。
- 方法层：OpenCompass 采用客观评测与主观评测两种评测方式。客观评测能便捷地评估模型在具有确定答案（如选择，填空，封闭式问答等）的任务上的能力，主观评测能评估用户对模型回复的真实满意度，OpenCompass 采用基于模型辅助的主观评测和基于人类反馈的主观评测两种方式。
- 工具层：OpenCompass 提供丰富的功能支持自动化地开展大语言模型的高效评测。包括分布式评测技术，提示词工程，对接评测数据库，评测榜单发布，评测报告生成等诸多功能。
### 2.4 OpenCompass工程流水线
![alt text](img6/OpenCompass工程流水线.png)
在 OpenCompass 中评估一个模型通常包括以下几个阶段：配置 -> 推理 -> 评估 -> 可视化。
- 配置：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。
- 推理与评估：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。推理阶段主要是让模型从数据集产生输出，而评估阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。如果需要了解该问题及解决方案，可以参考 FAQ: 效率。
- 可视化：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。 接下来，我们将展示 OpenCompass 的基础用法，展示书生浦语在 C-Eval 基准任务上的评估。它们的配置文件可以在 configs/eval_demo.py 中找到。
### 2.5 前沿探索
![alt text](img6/前沿探索-多模态.png)
![alt text](img6/前沿探索-垂直领域评测.png)

## 3 作业-基础任务
使用 OpenCompass 评测 internlm2-chat-1.8b 模型在 MMLU 数据集上的性能，记录复现过程并截图。
### 3.1 环境配置
创建开发机和 conda 环境，本次选择 GPU 为10% A100。
配置虚拟环境，安装opencompass：
```bash
studio-conda -o internlm-base -t opencompass
source activate opencompass
pip install protobuf tabulate mmengine tqdm prettytable datasets transformers jieba scikit-learn evaluate rouge_chinese nltk rank_bm25 sentence_transformers tiktoken absl-py fuzzywuzzy sentencepiece python-Levenshtein einops accelerate

cd /root/code
git clone -b 0.2.4 https://github.com/open-compass/opencompass
cd opencompass
pip install -e .
```
### 3.2 数据准备
解压评测数据集,将会在 OpenCompass 下看到data文件夹
```bash
cp /share/temp/datasets/OpenCompassData-core-20231110.zip /root/code/opencompass/
unzip OpenCompassData-core-20231110.zip
```
查看支持的数据集和模型,列出所有跟 InternLM 及 C-Eval 相关的配置
```bash
python tools/list_configs.py internlm ceval
```
![alt text](img6/查看支持的数据集和模型.png)
### 3.3 启动评测
通过以下命令评测 InternLM2-Chat-1.8B 模型在 C-Eval 数据集上的性能。由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 --debug 模式启动评估，并检查是否存在问题。在 --debug 模式下，任务将按顺序执行，并实时打印输出。
```bash
cd /root/code/opencompass/
export MKL_SERVICE_FORCE_INTEL=1
python run.py --datasets ceval_gen --hf-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True --model-kwargs trust_remote_code=True device_map='auto' --max-seq-len 1024 --max-out-len 16 --batch-size 2 --num-gpus 1 --debug
```
![alt text](img6/启动评测过程日志.png)
![alt text](img6/评测执行结果.png)
- outputs/default/ 目录下保存了评测结果
    - outputs/default/时间戳/configs 总结运行得config参数
    - outputs/default/时间戳/logs 保存评测过程中的日志信息
    - outputs/default/时间戳/predictions 保存模型得预测结果
    - outputs/default/时间戳/results 保存模型评测结果
    - outputs/default/时间戳/summary 保存模型评测结果的总结
![alt text](img6/评测结果保存的文件目录结构.png)
### 3.4 启动命令字段含义
    python run.py
    --datasets ceval_gen \
    --hf-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \  # HuggingFace 模型路径
    --tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \  # HuggingFace tokenizer 路径（如果与模型路径相同，可以省略）
    --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \  # 构建 tokenizer 的参数
    --model-kwargs device_map='auto' trust_remote_code=True \  # 构建模型的参数
    --max-seq-len 1024 \  # 模型可以接受的最大序列长度
    --max-out-len 16 \  # 生成的最大 token 数， 如果是多轮对话场景就要设置更大
    --batch-size 2  \  # 批量大小
    --num-gpus 1  \# 运行模型所需的 GPU 数量
    --work-dir '/root/temp/opencompass' \  # 保存评估结果过程信息的目录，默认在outputs/default下
    --reuse latest \ # 断点续存，latest可以替换成工作目录outputs/default下得某个时间戳文件，指定文件进行续存
    --debug  # 写该参数控制台输出， 不写该参数会输出到outputs/logs/ 目录下


## 参考资料
- https://github.com/InternLM/Tutorial/blob/camp2/opencompass/readme.md
- https://github.com/open-compass/opencompass/blob/main/README_zh-CN.md
- https://www.bilibili.com/video/BV1Pm41127jU/?spm_id_from=autoNext&vd_source=41bb3262014ad3cc41c3d25409df19be